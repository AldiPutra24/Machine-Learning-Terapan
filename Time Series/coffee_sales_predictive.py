# -*- coding: utf-8 -*-
"""Coffee Sales - Predictive.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/146AexrzphHZ62braUMJrmHqPHIRl1Wou

### Impor Library dan Paket yang Diperlukan
"""

!pip install -q pmdarima
import itertools

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import train_test_split
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX
from pmdarima import auto_arima
warnings.filterwarnings('ignore')

"""### Mengunduh Dataset"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("ihelon/coffee-sales")

print("Path to dataset files:", path)

df = pd.read_csv("/root/.cache/kagglehub/datasets/ihelon/coffee-sales/versions/13/index.csv")

"""### Data Understanding"""

# Menampilkan seluruh data yang ada di dalam dataframe
df

# Menampilkan informasi umum tentang dataframe, termasuk jumlah baris, kolom, tipe data, dan jumlah data non-null per kolom
df.info()

# Menampilkan statistik deskriptif dari data numerik pada dataframe
df.describe()

# Menampilkan jumlah nilai yang hilang (null) pada setiap kolom
df.isnull().sum()

# Menampilkan jumlah duplikasi dalam dataset
df.duplicated().sum()

"""Secara keseluruhan, bagian Data Understanding ini akan memberikan gambaran menyeluruh tentang struktur data, adanya data yang hilang atau duplikat, serta statistik dasar yang akan membantu dalam proses pembersihan dan persiapan data untuk tahap modeling.

### Data Preparation

Kode dibawah ini bertujuan untuk memastikan bahwa dataset memiliki tanggal yang lengkap dari tanggal paling awal hingga paling akhir, termasuk tanggal tanpa transaksi. Pertama, kode membuat rentang tanggal lengkap (date_range) dan menyimpannya dalam DataFrame complete_dates. Kemudian, dataset asli dikelompokkan berdasarkan tanggal dan dihitung jumlah transaksi per tanggal, disimpan dalam DataFrame df_by_date. Selanjutnya, kedua DataFrame tersebut digabung menggunakan left join untuk memastikan semua tanggal tercakup, meskipun tidak ada transaksi pada beberapa tanggal. Terakhir, nilai NaN pada tanggal tanpa transaksi diganti dengan nol untuk menjaga integritas data saat analisis lebih lanjut.

Hasilnya adalah DataFrame df_complete yang berisi setiap tanggal beserta jumlah transaksi harian, dengan nilai nol untuk hari tanpa transaksi.
"""

# Mengonversi kolom 'date' menjadi tipe datetime agar lebih mudah dalam manipulasi data waktu
df["date"] = pd.to_datetime(df["date"])

# Membuat rentang tanggal dari tanggal terendah hingga tertinggi pada data yang ada
date_range = pd.date_range(start=df["date"].min(), end=df["date"].max())

# Membuat DataFrame baru dengan rentang tanggal lengkap untuk mengisi data yang mungkin hilang
complete_dates = pd.DataFrame(date_range, columns=["date"])

# Mengelompokkan data berdasarkan 'date' dan menghitung jumlah transaksi (cups) per hari
df_by_date = df.groupby("date").agg({"money": ["count"]}).reset_index()

# Mengganti nama kolom menjadi 'cups' untuk menunjukkan jumlah transaksi/hari
df_by_date.columns = ["date", "cups"]

# Menggabungkan dataframe 'complete_dates' dengan 'df_by_date' untuk memastikan semua tanggal ada,
# dan mengisi tanggal tanpa transaksi dengan 0
df_complete = pd.merge(complete_dates, df_by_date, on="date", how="left")
df_complete.fillna(0, inplace=True)

# Menampilkan dataframe hasil untuk memeriksa apakah ada missing date yang telah terisi
df_complete.head()

# Menghitung korelasi antar kolom pada dataframe df_complete
corr = df_complete.corr()

# Menyusun plot heatmap untuk matriks korelasi
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True)

"""Terdapat korelasi antara kolom money dan cups, itu menunjukkan bahwa ada hubungan total uang yang dibelanjakan dengan jumlah kopi yang terjual.

#### Split data menjadi set pelatihan dan pengujian:
"""

# Menentukan ukuran data pengujian (test size)
test_size = 7

# Menentukan ukuran data pelatihan (train size)
train_size = df_complete.shape[0] - test_size

# Memisahkan data menjadi dua set: df_train untuk pelatihan dan df_test untuk pengujian
df_train = df_complete.iloc[:train_size]
df_test = df_complete.iloc[train_size:]

# Menyiapkan list untuk menyimpan hasil evaluasi (metrics)
metrics = []

"""Kode ini memisahkan dataset menjadi dua bagian: data pelatihan (df_train) dan data pengujian (df_test), yang digunakan untuk melatih dan menguji model secara terpisah.
Ini adalah langkah yang penting untuk menghindari overfitting dan untuk memvalidasi model dengan data yang tidak terlihat sebelumnya.
"""

plt.figure(figsize=(14, 6))
sns.lineplot(data=df_train, y="cups", x="date", label="Train")
sns.lineplot(data=df_test, y="cups", x="date", label="Test")
plt.grid()
plt.ylim(0)

"""Kode ini membuat grafik garis untuk memvisualisasikan data pelatihan dan pengujian secara terpisah, yang memungkinkan kita untuk melihat tren penjualan cangkir selama periode waktu tertentu dan membandingkan pola penjualan antara data pelatihan dan pengujian.
Dengan menggunakan grid dan pengaturan batas Y, grafik ini lebih mudah dibaca dan memberi gambaran yang lebih jelas tentang tren data.

### Modeling

#### ARIMA
"""

# Menentukan parameter p, d, q untuk model ARIMA
p, d, q = 5, 0, 5  # p = order autoregressive, d = order differencing, q = order moving average

# Membuat model ARIMA dengan data pelatihan 'cups' dan parameter yang sudah ditentukan
model = ARIMA(df_train['cups'], order=(p, d, q))

# Melatih model ARIMA dengan data pelatihan
model_fit = model.fit()  # Model ARIMA di-fit dengan data pelatihan untuk mendapatkan parameter terbaik

# Menggunakan model yang sudah dilatih untuk memprediksi nilai di data pengujian
test_predictions= model_fit.forecast(steps=len(df_test)).values  # Prediksi untuk data pengujian (steps = panjang data test)

# Menyimpan hasil prediksi dalam kolom baru 'arima_pred' di data pengujian
df_test["arima_pred"] = test_predictions # Menambahkan prediksi model ke df_test untuk analisis lebih lanjut

"""- Menetapkan nilai untuk parameter ARIMA (p, d, dan q).
- Membangun model ARIMA menggunakan data pelatihan (df_train['cups']) dengan parameter yang sudah ditetapkan.
- Melatih model menggunakan fungsi fit() untuk mendapatkan parameter yang optimal.
- Menggunakan model yang telah dilatih untuk menghasilkan prediksi sebanyak data pada set pengujian.
- Menyimpan prediksi ke dalam kolom baru di df_test agar bisa dibandingkan dengan nilai aktual di data pengujian.
"""

mae = mean_absolute_error(df_test["cups"], test_predictions)
metrics.append({"model": "ARIMA", "mae": mae})
print(f'Mean Absolute Error: {mae}')

"""#### AUTO SARIMAX"""

# Membangun model AutoARIMA dengan parameter untuk data musiman
model = auto_arima(
    df_train['cups'],  # Menggunakan data pelatihan 'cups' untuk membangun model
    seasonal=True,  # Menentukan bahwa data memiliki komponen musiman
    m=7,  # Menentukan panjang musim (7 hari dalam seminggu)
    trace=True,  # Menampilkan proses pencarian parameter terbaik (trace)
    error_action='ignore',  # Mengabaikan kesalahan selama pencarian parameter
    suppress_warnings=True,  # Menekan peringatan yang tidak perlu selama pelatihan
)

# Menggunakan model AutoARIMA yang sudah dilatih untuk memprediksi data pengujian
test_predictions = model.predict(n_periods=len(df_test)).values  # n_periods = panjang data test untuk prediksi

# Menyimpan hasil prediksi dalam kolom baru 'auto_sarimax_pred' di data pengujian
df_test["auto_sarimax_pred"] = test_predictions # Menambahkan prediksi model ke df_test untuk analisis lebih lanjut

# Menyimpan hasil prediksi dalam kolom baru 'auto_sarimax_pred' di data pengujian
df_test["auto_sarimax_pred"] = test_predictions # Menambahkan prediksi model ke df_test untuk analisis lebih lanjut

"""- Membangun model AutoARIMA: Kode ini menggunakan fungsi auto_arima untuk membangun model yang mengoptimalkan parameter ARIMA secara otomatis, termasuk komponen musiman. Parameter m=7 menandakan data musiman dengan periode 7 hari (misalnya, pola mingguan).
- Pengaturan tambahan: trace=True menampilkan proses pencarian parameter terbaik selama pelatihan, sementara error_action='ignore' dan suppress_warnings=True digunakan untuk mengabaikan kesalahan atau peringatan selama proses pelatihan.
- Prediksi dengan model AutoARIMA: Setelah model dilatih, fungsi predict(n_periods) digunakan untuk memprediksi nilai pada data pengujian (df_test) selama periode yang diperlukan (n_periods).
- Menyimpan hasil prediksi: Prediksi yang dihasilkan disimpan dalam kolom baru auto_sarimax_pred pada data pengujian (df_test) agar bisa dianalisis lebih lanjut.
"""

mae = mean_absolute_error(df_test["cups"], test_predictions_sarimax)
metrics.append({"model": "Auto SARIMAX", "mae": mae})
print(f'Mean Absolute Error: {mae}')

"""### Evaluasi & Visualisasi Model

#### ARIMA
"""

# Visualisasi data train, test, dan prediksi ARIMA
plt.figure(figsize=(14, 6))  # Menentukan ukuran grafik
sns.lineplot(data=df_train, y="cups", x="date", label="Train")  # Menampilkan data training
sns.lineplot(data=df_test, y="cups", x="date", label="Test")  # Menampilkan data testing
sns.lineplot(data=df_test, y="arima_pred", x="date", label="Predictions")  # Menampilkan hasil prediksi ARIMA
plt.title('ARIMA')  # Menambahkan judul grafik
plt.grid()  # Menambahkan grid untuk memudahkan visualisasi
plt.ylim(0)  # Membatasi sumbu y mulai dari 0

"""Pada bagian ini, grafik garis menampilkan data pelatihan (Train), data pengujian (Test), dan hasil prediksi model ARIMA (Predictions) berdasarkan data pengujian.
Fungsi sns.lineplot() digunakan untuk menggambar grafik garis dengan label yang sesuai untuk membedakan setiap data (Train, Test, dan Prediksi).

#### Auto SARIMAX
"""

# Visualisasi data train, test, dan prediksi Auto SARIMAX
plt.figure(figsize=(14, 6))  # Menentukan ukuran grafik
sns.lineplot(data=df_train, y="cups", x="date", label="Train")  # Menampilkan data training
sns.lineplot(data=df_test, y="cups", x="date", label="Test")  # Menampilkan data testing
sns.lineplot(data=df_test, y="auto_sarimax_pred", x="date", label="Auto SARIMAX Predictions")  # Menampilkan hasil prediksi Auto SARIMAX
plt.title('SARIMA')  # Menambahkan judul grafik
plt.grid()  # Menambahkan grid untuk memudahkan visualisasi
plt.ylim(0)  # Membatasi sumbu y mulai dari 0

"""Sekali lagi, grafik yang serupa digambar, tetapi kali ini menggunakan model Auto SARIMAX untuk memprediksi data pengujian. auto_sarimax_pred menampilkan hasil prediksi yang dihasilkan oleh model tersebut.
Ini memberi gambaran bagaimana model Auto SARIMAX dibandingkan dengan model ARIMA dalam memprediksi jumlah penjualan.

#### ARIMA VS Auto SARIMAX
"""

df_test

"""Menampilkan dua kolom prediksi, yaitu dari model ARIMA (arima_pred) dan Auto SARIMAX (auto_sarimax_pred)."""

# Membandingkan hasil metrik antara ARIMA dan Auto SARIMAX
df_metrics = pd.DataFrame(metrics)  # Membuat DataFrame dari metrics yang sudah dihitung sebelumnya

# Visualisasi perbandingan metrik MAE antara ARIMA dan Auto SARIMAX
plt.figure(figsize=(8, 1 * df_metrics.shape[0] // 2))  # Mengatur ukuran grafik barplot
sns.barplot(data=df_metrics, y="model", x="mae")  # Menampilkan barplot berdasarkan MAE dari masing-masing model

"""Bagian ini membandingkan kinerja model ARIMA dan Auto SARIMAX menggunakan metrik Mean Absolute Error (MAE).
df_metrics berisi data metrik (seperti MAE) dari kedua model. Barplot digunakan untuk menggambarkan hasilnya dengan jelas, dengan mae sebagai sumbu X dan model sebagai sumbu Y.

Perbandingan MAE:

ARIMA: Memberikan MAE yang menggambarkan rata-rata kesalahan absolut.
Auto SARIMAX: Diharapkan memberikan MAE lebih rendah karena model ini mengoptimalkan parameter musiman.
Visualisasi Hasil Prediksi:

Grafik prediksi menunjukkan perbedaan antara model ARIMA dan Auto SARIMAX.
Model dengan prediksi lebih mendekati data aktual dianggap lebih baik.
Perbandingan MAE:

Perbandingan MAE antara kedua model dapat dilakukan untuk menentukan model terbaik berdasarkan nilai MAE yang lebih rendah.
Kesimpulan:

Model dengan MAE lebih rendah lebih baik dalam memprediksi penjualan. Pilihan model juga mempertimbangkan kesederhanaan dan biaya komputasi.
"""